%% \documentclass[11pt]{article}

%% \textwidth 6.5in
%% \textheight 8.5in
%% \topmargin -0.25in
%% \evensidemargin 0.00in
%% \oddsidemargin 0.00in

%% \usepackage{amssymb}
%% \usepackage{amsmath}
%% \usepackage{latexsym,palatino}
%% %\usepackage{lucidbry}
%% %\usepackage{tightlist}
%% \usepackage{url}
%%  \linespread{1.1}    
%%  \usepackage[colorlinks=true,urlcolor=blue,citecolor=blue,linkcolor=blue]{hyperref}              % For pdf and postscript files


%% \pagestyle{plain}

%% \begin{document}

\begin{center}
%{\sc\large\bf FACILITIES, EQUIPMENT, AND OTHER RESOURCES} \\
{\sc \bf FERMI NATIONAL ACCELERATOR LABORATORY}\\
\end{center}

{\bf Fermi National Accelerator Laboratory} provides facilities computing resources in support of it's HEP mission, for archival data storage, high throughput (grid) computing and networking.  

The Fermilab high throughput grid computing facilities operates computing systems providing 19,664 x86 based computing cores with an annual capacity to provide approximately 172 million CPU hours.  The facility provides gigabit Ethernet connectivity within the grid clusters and provides higher aggregated bandwidth paths to central storage facilities.   The facility is shared across Fermilab hosted and associated experiments through a HTCondor based batch submission system.  This system permits the central data processing/analysis teams of each experiment to conduct large scale simulation and data analysis on their experiment's datasets.  The facility also provides the capabilities to allow individual experimenters associated with Fermilab experiments to submit their workloads and receive back their results.  This facility serves as the basis for current FNAL analysis work for intensity frontier experiments (NOvA, MicroBooNE, DUNE) and for the USCMS Tier 1 facility.

The computing capabilities of the Fermilab facility are augmented by the storage systems that the laboratory operates.  The storage facility features four, ten thousand (10,000) slot Oracle SL8500 robotic tape libraries with an additional 3 tape libraries dedicated to the CMS experiment.  The storage facility operates 69 tape drives supporting T10Kc, T10Kd, and LTO4 media.  The facility has on the order of 15,000 active media cartridges with an additional 16,000 slots occupied by data migration processes. The facility archival storage facility's tape systems are fronted by a distributed disk caching system with provides 3.4~PB of read/write cache buffer from/to tape, a 1.4~PB non-tape backed cache for large scale data analysis operations.  This facility is the standard data archive and repository for data for the NOvA, MicroBooNE, DUNE and other intensity frontier experiments.  The facility is also the storage base of the CMS Tier 1 facility. 

Through a grant from the National Energy Research Scientific Computing Center (NERSC) the Fermilab Scientific Computing Division has an allocation of 16 million hours on the CORI HPC facility.  Of the base allocation up to 500 thousand hours would be available to this proposal for performing studies and integration work specific to the CORI design architecture and the Knights Landing based CORI-2 facility.

%% \end{document}
