\label{sec:overview}
The objective of the computing and software project in ATLAS is to enable the Collaboration to fully exploit the HL-LHC dataset, whilst limiting the costs associated with computing. This Conceptual Design Report (CDR) summarises the current approaches being taken and plans being laid within ATLAS to meet this challenge. In particular areas where greater investment will be required in the coming years are highlighted. Based on the present Computing Model, an extrapolation of the resources requirements needed for HL-LHC indicates a significant shortfall in disk and compute power. This is true even if the investment from funding agencies continues to increase at the rate established in  previous years. A significant investment in person power to carry out the necessary research and development to address this shortfall and thereby avoid excessive computing costs in the future. The long term sustainability of the ATLAS software therefore depends on engaging individuals whose expertise and specialism is software development. Given the complexity of the ATLAS detector and software project, sustainability over the next two decades will require that senior decision makers and funding agencies are willing to invest in people, and award personal research grants to those who whose specialism in in software and computing development. A more modern and open software architecture will help attract early career researchers with a software background, but sustainability requires that such individuals have a realistic prospect of long term employment in the field. \\
 
To sustain HL-LHC data analysis, Monte Carlo events will need to be generated and simulated in prodigious numbers - it is likely that approximately ten times more MC statistics will be required during Run 4, which implies at least 200 billion events. \\

Event generation for ATLAS has been a large consumer of compute resources to date (approximately 10-15\%) and it is likely to become more prominent in Run 4, as many measurements and searches will require higher precision, which implies use of NLO or NNLO generators. As well as software development to maximise efficiency and perhaps make use of accelerators, careful optimisations of physics choices, bias the event generation as a function of a kinematic quantity of interest (thereby reducing the number of events needed), suppressing negative event weights, and even sharing generated events between collaborations, will all contribute to ensuring that event generation is sustainable in Run 4. It should be noted that event generators are typically developed by small teams of theorists, whose primary concerns are not related to computing. Endeavours to increase collaboration on software across the field, especially the High Energy Physics Software Foundation (HSF), are of crucial importance if applications such as event generation are to be scaled up to HL-LHC demands. \\ 

ATLAS currently expends most (60\%) of its CPU resources on detector simulation. Approximately half of the events are produced with so-called full simulation, using the Geant4 physics library, with the rest coming from fast simulation which uses a parameterised model of the calorimeter response. Full simulation uses around eight times more compute resources than fast. ATLAS aims to raise the level of fast simulation to 75\% at the very least during Run 4. However, ATLAS will need to extend the fast simulation concept to all parts of the detector, including the inner tracker as well as the calorimeter. Novel simulation techniques made possible by deep learning may also prove highly effective at speeding up aspects of the simulation. Use of full simulation will remain unavoidable for certain applications, and for this reason ATLAS will collaborate closely with the Geant4 team to ensure the best possible physics fidelity for the lowest possible expenditure of resources. Finally, ATLAS must ensure that the compute costs of trigger simulation remain under control, especially as hardware based tracking will be used in the HL-LHC trigger system. \\

The reconstruction stage, which is fully under the responsibility of ATLAS, will benefit enormously from the Phase II upgrade to the inner tracker, the ITk. As well as providing exceptional physics performance, it is also optimised to minimise CPU consumption of tracking despite the much higher pile-up at the HL-LHC. At the same time, ATLAS is pursuing several promising avenues of software optimisation, which together with the ITk should ensure sustainable reconstruction in Run 4. These developments include improvements and optimisations to the classical algorithms that can further exploit the ITk design and provide very substantial CPU savings with tolerable loss of physics performance. ATLAS is also implementing a new cross experiment tracking software, which benefits from a modern design with a light-weight event data model. This is expected to yield further savings and efficiencies. (write something here quantitative). Novel approaches based on machine learning, or new non-ML algorithms, may be able to improve the physics and computing performance further. Finally, ATLAS is investigating the possibility of allowing accelerators to be used for aspects of the reconstruction, as part of the heterogeneous software programme. All of these improvements require a determined investment to maintain and increase the available person power. \\

ATLAS is implementing a new analysis model which aims to reduce the disk footprint occupied by analysis formats. It is based around two formats, one with a size of at most 50KB/event and the other 10KB/event. The first of these is primarily aimed at Run 3 analyses; it contains all of the variables needed to apply calibrations to reconstructed objects. This will replace a large number of dedicated analysis formats currently in use during Run 2, and approximately halve the disk footprint of the analysis formats. The second format is aimed at Run 4. The reconstructed objects represented in this small format are pre-calibrated, and in consequence the variables needed to apply the calibrations do not need to be stored. This format will be commissioned in Run 3 for use in Run 4, with the main challenge being the assessment of systematics with such a small format. Another important piece of the new analysis model is the appropriate application of lossy compression, so that variables stored in reconstruction and analysis data formats are stored with a precision concomitant with the instrumental precision. Approximately 10\% size reduction could be made to all formats storing reconstructed data objects, should this be maximally utilised. \\

Storage is a more significant challenge than compute at HL-LHC for the obvious reason that CPU is volatile, whereas once a disk or tape is full, it remains full until data is deleted. Additionally opportunistic computing resources exist, but not opportunistic storage. Significant optimisations can be made to fully exploit the available hardware. In particular the data carousel can reduce by half the total AOD volume permanently resident on disk, staging AOD files from tape to a disk buffer when they are required for processing. This activity is both an investment in person-power and hardware, since it involves optimising and tailoring data movement site by site, making the best possible use of continuously evolving infrastructure. Expectation management is also an important part of reducing storage costs, since by organizing the processing campaigns with well defined timelines and reducing the amount of on-demand production, the amount of data pinned to expensive disk resources can be further reduced. \\

The backbones of the data processing and management - Panda and Rucio - will need to be scaled up to HL-LHC workloads and volumes. Intelligent scheduling of jobs based on improved coupling between job characteristics and available resources, for instance CPU and accelerators, or I/O intensive jobs scheduled on nodes with storage with high I/O capability, is important to allow full exploitation of the heterogeneous resources that are likely be provided in the coming years. Evolution of the already highly efficient WLCG infrastructure in this direction is anticipated. Intelligent automation coupled with new emerging technologies (such as containers and infrastructure state of the art deployment tools)  will allow a reduction in the workloads on staff at sites, whilst maintaining the efficiency of the Grid. Use of new storage technologies, such as cloud storage, data lakes and caches, might also yield further benefits especially for smaller sites. This would save operational efforts running a cache, which is much simpler than normal grid storage. R\&D activities in this area are paramount, and for such activity we need engagement of the whole WLCG.

The general trend of the evolution of the IT industry is towards more heterogeneity in computational hardware, especially with regards to accelerators such as GPUs. These changes are driven particularly by advances in data science and machine learning. Although such resources are today most commonly found in very large High Performance Computing centres (HPCs) that are not primarily built for high energy physics, by the time of HL-LHC it is possible that hardware installed at dedicated WLCG sites will feature elements of these technologies as well. In order to make full use of all of the resources available at the time, it is clear that the software stacks used by HEP experiments must by then have evolved such that they can run efficiently on such devices, whilst still being compatible with more familiar hardware. The recent transition by ATLAS to a multi-threaded framework is a necessary first step towards this aim.\\

\subsection{Resources requirement modelling}
To inform discussion of the resources needed for HL-LHC computing, projections need to be made based on reasonable assumptions as to the activities that will be carried out by ATLAS, the actions it will take to minimize computing costs, and the provision from the funding agencies. For the purposes of this document, ATLAS has chosen to evaluate three scenarios, as follows:
\begin{itemize}
    \item \textbf{Conservative}: ATLAS implements the new data formats foreseen by the Run 3 analysis model, and the multi-threaded software framework AthenaMT, but otherwise continues in largely the same way as in Run 2 - in particular the CPU time per event is assumed to scale in the same way with pile-up as in Run 2, and the mixture of generators and simulation remains the same; 
    \item \textbf{Baseline}: the research and development activities currently under way are assumed to be successful, in particular the data carousel, fast chain simulation and lossy compression; 
    \item \textbf{Aggressive}: ATLAS implements very aggressive resources reduction measures that would substantially reduce the computing resources, but at the price of reducing physics reach or precision - examples would include using only the 10KB/event analysis format in Run 4, using almost no full simulation, reducing the use of high-precision N(N)LO generators, scaling back MC statistics or not writing the full output of reconstruction (AOD) at all. Alternatively, this scenario could include developments that very significantly improve the speed or storage volumes of workflows that currently are heavy consumers of resources - for example, porting of high-precision generators (such as Sherpa) to GPUs, sharing events with CMS, making use of MC-MC overlay, or speeding up the full simulation either by software efficiencies or porting parts of the code to GPUs.
\end{itemize}
